## Results
Adam has higher maximums but SGD seems better on average.
Adam needs a low learning rate, 0.001 is good. 

## TODO:
Test ConvNet with pooling vs strided convolution
Try Adam with-and-without schedulers again