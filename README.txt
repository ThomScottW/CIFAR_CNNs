## Results
Adam has higher maximums but SGD seems better on average.
Adam needs a low learning rate, 0.001 is good. 

## TODO:
Re-run Adam Batchnorm comparison
Test ConvNet with pooling vs strided convolution